{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c2ec43-e80f-4746-8636-325235e3c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sam Brown\n",
    "# sam_brown@mines.edu\n",
    "# June19\n",
    "# Goal: Preprocess data and create dataframe for analysis of long-term slip patterns\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import my_lib.funcs\n",
    "import Stations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# NOTES use GZ05 for coordinates to retrieve tide data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dcb575a-2b05-497a-a09c-897d5f2c8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the paths\n",
    "\n",
    "# df_2008 = my_lib.funcs.load_evt(\"/Users/sambrown/Documents/SURF/Events/2008_2008Events2stas\")\n",
    "# df_2009 = my_lib.funcs.load_evt(\"/Users/sambrown/Documents/SURF/Events/2009_2009Events2stas\")\n",
    "df_2010 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2010_2010Events2stas\")\n",
    "df_2011 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2011_2011Events2stas\")\n",
    "df_2012 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2012_2012Events2stas\")\n",
    "df_2013 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2013_2013Events2stas\")\n",
    "df_2014 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2014_2014Events2stas\")\n",
    "df_2015 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2015_2015Events2stas\")\n",
    "df_2016 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2016_2016Events2stas\")\n",
    "df_2017 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2017_2017Events2stas\")\n",
    "df_2018 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2018_2018Events2stas\")\n",
    "df_2019 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2019_2019Events2stas\")\n",
    "\n",
    "# One Large list\n",
    "all_dfs = (\n",
    "    df_2010 + df_2011 + df_2012 + df_2013 +\n",
    "    df_2014 + df_2015 + df_2016 + df_2017 + df_2018 + df_2019\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "clean_df = my_lib.funcs.extract_event_features(all_dfs)\n",
    "\n",
    "standards = pd.read_csv('../station_standards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16b81147-1ccd-4a8f-943d-9c9811645973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through each event. Loop through each station in events, append the standardized delta to the list then average it.\n",
    "# add start time to data frame\n",
    "\n",
    "# Initialize dataframe\n",
    "net_df = pd.DataFrame(columns = [ 'tide_deriv', 'form_fac', 'time_since', 'slip_size_standardized', 'high_t_evt', 'start_time', 'sev_stds', 'pre-s_stds']) # Tide height will be added by merge\n",
    "\n",
    "for event in clean_df:\n",
    "\n",
    "    # Initialize list for slip sizes\n",
    "    slip_deltas = []\n",
    "    slip_sevs = []\n",
    "    slip_pre = []\n",
    "\n",
    "    # Loop through rows\n",
    "    for i, row in event.iterrows():\n",
    "        station = row['station'][:4]\n",
    "        if station == 'slw1' or station =='ws04' or station =='ws05':\n",
    "            continue\n",
    "\n",
    "        row_sch = standards[standards['Station'] == station]\n",
    "\n",
    "        # if row_sch.empty:\n",
    "        #     raise ValueError(f\"Station not found in standards: '{station}'\")\n",
    "        \n",
    "        # Standardization size\n",
    "        station_mean = row_sch['slip_size'].iloc[0]\n",
    "        station_sd = row_sch['slip_size_sd'].iloc[0]\n",
    "        standardized_val = (row['total_delta'] - station_mean) / station_sd\n",
    "        # print(f\"{row['total_delta']}, {station}, {station_mean}, {station_sd}\")\n",
    "\n",
    "        # Standardization slip severity\n",
    "        station_sev_u = row_sch['slip_severity'].iloc[0]\n",
    "        station_sev_o = row_sch['slip_severity_sd'].iloc[0]\n",
    "\n",
    "        standardized_val_sev = (row['slip_severity'] - station_sev_u) / station_sev_o\n",
    "\n",
    "        # Standardization pre-slip area\n",
    "        station_slip_u = row_sch['pre-slip_area'].iloc[0]\n",
    "        station_slip_o = row_sch['pre-slip_area_sd'].iloc[0]\n",
    "\n",
    "        standardized_val_slip = (row['pre-slip_area'] - station_slip_u) / station_slip_o        \n",
    "\n",
    "        slip_deltas.append(standardized_val)\n",
    "        slip_sevs.append(standardized_val_sev)\n",
    "        slip_pre.append(standardized_val_slip)\n",
    "        \n",
    "\n",
    "    net_df.loc[len(net_df)] = {\n",
    "        \"slip_size_standardized\": sum(slip_deltas) / len(slip_deltas),\n",
    "        \"start_time\": event.at[0, 'start_time'],\n",
    "        \"sev_stds\": sum(slip_sevs) / len(slip_sevs),\n",
    "        \"pre-s_stds\": sum(slip_pre) / len(slip_pre)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4880cf5f-ab25-4874-854a-10e1e63d8568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 44.83895397186279 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load tide Data\n",
    "# NOTE: previously averaged coors of gz stations and had missing vals; will use gz05 now\n",
    "\n",
    "#Average coordinates for gz stations (source code in severity_class nb)\n",
    "# x_cor = -168955.1491394913 \n",
    "# y_cor = -599694.5432784811\n",
    "\n",
    "# GZ05 coors\n",
    "x_cor = -155992.359664\n",
    "y_cor = -604863.615642\n",
    "\n",
    "tide_df = my_lib.funcs.get_tide_height(4380, x_cor, y_cor, \"2008-01-01 00:00:00\") # 12 years worth of data\n",
    "\n",
    "# Organize events by time\n",
    "net_df = net_df.sort_values('start_time')\n",
    "\n",
    "# Calculate time since in minutes\n",
    "net_df['time_since'] = net_df['start_time'].diff().dt.total_seconds() / 60\n",
    "\n",
    "# Get start times down to minutes\n",
    "tide_df['time'] = tide_df['time'].apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "net_df['start_time'] = net_df['start_time'].apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "\n",
    "# Insert Tide values\n",
    "net_df['start_time'] = pd.to_datetime(net_df['start_time'])\n",
    "tide_df['time'] = pd.to_datetime(tide_df['time'])\n",
    "\n",
    "# Merge tide height into net_df based on matching timestamps\n",
    "merged_df = pd.merge(net_df, tide_df[['time', 'tide_height']], \n",
    "                     left_on='start_time', right_on='time', how='left')\n",
    "\n",
    "# Drop extra 'time' column if you want\n",
    "merged_df = merged_df.drop(columns=['time'])\n",
    "\n",
    "# Insert tide derivatives into data\n",
    "tide_d = my_lib.funcs.tide_derivative(tide_df)\n",
    "for i, row in merged_df.iterrows():\n",
    "    time = row['start_time']\n",
    "\n",
    "    index = tide_d[tide_d['time'] == time].index\n",
    "\n",
    "    if not index.empty:\n",
    "        idx = index[0]\n",
    "        merged_df.at[i, 'tide_deriv'] = tide_d.at[idx, 'tide_deriv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ef8577d-082f-4084-9a79-ed4db094205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sambrown04/Documents/SURF/whillans-surf/notebooks/SURF/data_preproc/../my_lib/funcs.py:411: OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "  popt, pcov = scipy.optimize.curve_fit(sines, seconds_tide, tide_window, p0=initial_guess)\n"
     ]
    }
   ],
   "source": [
    "# Form factor calculation\n",
    "\n",
    "form_fac = my_lib.funcs.form_factor_calc(tide_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c750c7ae-ae04-423f-bbbb-b171e040b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date-only column to form_fac\n",
    "form_fac['date_only'] = form_fac['dates'].dt.date\n",
    "\n",
    "# Loop through each row in avg_dat\n",
    "for i, event in merged_df.iterrows():\n",
    "    time = event['start_time']\n",
    "    target_date = time.date()\n",
    "\n",
    "    # Select all rows with matching date\n",
    "    rows_date = form_fac[form_fac['date_only'] == target_date]\n",
    "\n",
    "    # Compute average form factor for that date\n",
    "    merged_df.at[i, 'form_fac'] = rows_date['form_factors'].mean()\n",
    "\n",
    "# Encode high tide vs low tide event\n",
    "merged_df['high_t_evt'] = (merged_df['tide_height'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "743b425e-6819-4fa7-9a65-d28b3108ad59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We would like to add feature(s) that takes the time period between events and retrieves the form factor for the time period between these events. \n",
    "# Two different features, semi diurnal fit and diurnal fit \n",
    "\n",
    "# Loop through each event \n",
    "for i, evt in merged_df.iterrows():\n",
    "\n",
    "    # Can't find time before first event\n",
    "    if i < 1:\n",
    "        continue\n",
    "    \n",
    "    # get start time and duration minutes\n",
    "    curDate = merged_df.at[i, 'start_time']\n",
    "    prevDate = merged_df.at[i-1, 'start_time']\n",
    "\n",
    "    diff = curDate - prevDate\n",
    "    minutes = diff.total_seconds() / 60.0\n",
    "\n",
    "    # calculate form factor for this time period\n",
    "    [form_fac, A1, A2, phi1, phi2] = my_lib.funcs.form_factor_window(tide_df, prevDate, minutes)\n",
    "    # print([form_fac, A1, A2, phi1, phi2])\n",
    "\n",
    "    # insert the features into the data set for that row\n",
    "    merged_df.loc[i, 'inter_form_Fac'] = form_fac\n",
    "    merged_df.loc[i, 'A_diurn'] = A1\n",
    "    merged_df.loc[i, 'A_semidiurn'] = A2\n",
    "    # merged_df.loc[i, 'phi1'] = phi1\n",
    "    # merged_df.loc[i, 'phi2'] = phi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97a0bfed-afb0-4d9f-beb7-d6608afdee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv file\n",
    "\n",
    "merged_df.to_csv(\"/Users/sambrown04/Documents/SURF/Preproc_data/10-18.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035005d-6756-4840-9aa5-16d6634813f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
