{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d202956-08fe-4bce-9fae-50c0fc6d46f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sam Brown\n",
    "# sam_brown@mines.edu\n",
    "# July 8\n",
    "# Goal: Investigate different ways to calculate the energy of a slip event and look for trends \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Stations import Station\n",
    "import my_lib.funcs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a30abb-db8d-46d5-9562-fd0ffd12fc4c",
   "metadata": {},
   "source": [
    "In this notebook I will be calculating the energy of a slip using a simple energy formula(s). \n",
    "$K = \\frac{1}{2}m v^2$\n",
    "There are existing formulas in the realm of seismology for calculating stress drop. For a circular fault in a whole space, we can estimate the stress with the equation\n",
    "$$\n",
    "\\Delta \\sigma = \\frac{7 \\pi \\mu \\bar{D}}{16r} = \\frac{7 M_0}{16r^3}\n",
    "$$\n",
    "Where r is the fault radius, $\\mu$ is the shear modulus, and $\\bar{D}$ is the average displacement. Overall, these equations are important (when simplified) as they model the idea that the shear stress change on the fault will be proportional to the ratio of the displacement to the size of the fault."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43152c2a-1ea0-4981-a73e-70993fbf43f9",
   "metadata": {},
   "source": [
    "For our Calculations we will start by investigating the energy of the events that occur in the grounding zone (GZ) area. First we will need to calculate of the volume of the ice in that area. We will use a circle with a radius of 15 km centered at gz07. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c50c035-75cf-4af9-b42f-18368241e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Antarctica ice depth data\n",
    "filepath = \"/Users/sambrown04/Documents/SURF/NSIDC-0756_3-20250708_204745/BedMachineAntarctica-v3.nc\"\n",
    "ds = xr.open_dataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c12c321-168f-414d-bf76-b89a38af1623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 4GB\n",
      "Dimensions:    (x: 13333, y: 13333)\n",
      "Coordinates:\n",
      "  * x          (x) int32 53kB -3333000 -3332500 -3332000 ... 3332500 3333000\n",
      "  * y          (y) int32 53kB 3333000 3332500 3332000 ... -3332500 -3333000\n",
      "Data variables:\n",
      "    mapping    |S1 1B ...\n",
      "    mask       (y, x) int8 178MB ...\n",
      "    firn       (y, x) float32 711MB ...\n",
      "    surface    (y, x) float32 711MB ...\n",
      "    thickness  (y, x) float32 711MB ...\n",
      "    bed        (y, x) float32 711MB ...\n",
      "    errbed     (y, x) float32 711MB ...\n",
      "    source     (y, x) int8 178MB ...\n",
      "    dataid     (y, x) int8 178MB ...\n",
      "    geoid      (y, x) int16 356MB ...\n",
      "Attributes: (12/17)\n",
      "    Conventions:                 CF-1.7\n",
      "    Title:                       BedMachine Antarctica\n",
      "    Author:                      Mathieu Morlighem\n",
      "    version:                     03-Jun-2022 (v3.4)\n",
      "    nx:                          13333.0\n",
      "    ny:                          13333.0\n",
      "    ...                          ...\n",
      "    ymax:                        3333000\n",
      "    spacing:                     500\n",
      "    no_data:                     -9999.0\n",
      "    license:                     No restrictions on access or use\n",
      "    Data_citation:               Morlighem M. et al., (2019), Deep glacial tr...\n",
      "    Notes:                       Data processed at the Department of Earth Sy...\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1ab728-2223-4f06-ab11-4565716aeee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gz07x = -168846.09042414097 \n",
    "gz07y = -602168.8273598101\n",
    "\n",
    "gz05x = -155766.98797266872 \n",
    "gz05y = -605038.2386205866\n",
    "\n",
    "gz01x = -178533.70585008597 \n",
    "gz01y = -610921.0441554557\n",
    "\n",
    "gz18x = -167907.0049366487 \n",
    "gz18y = -588200.1441347648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed96fc82-f8be-4846-a889-1114e4ccd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gz07: 730.12 meters\n",
      "gz05: 738.91 meters\n",
      "gz01: 759.08 meters\n",
      "gz18: 750.39 meters\n"
     ]
    }
   ],
   "source": [
    "coords = {\n",
    "    'gz07': (-168846.09042414097, -602168.8273598101),\n",
    "    'gz05': (-155766.98797266872, -605038.2386205866),\n",
    "    'gz01': (-178533.70585008597, -610921.0441554557),\n",
    "    'gz18': (-167907.0049366487, -588200.1441347648)\n",
    "}\n",
    "\n",
    "for label, (x_val, y_val) in coords.items():\n",
    "    thickness = ds['thickness'].sel(x=x_val, y=y_val, method='nearest')\n",
    "    print(f\"{label}: {thickness.values:.2f} meters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568f98a8-6d82-4bb3-9bd0-79114e5c3c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478976617182315.75\n"
     ]
    }
   ],
   "source": [
    "# Take an average of these points around our desired circle \n",
    "avg_depth = 744.63\n",
    "volume = np.pi * 15000 ** 2 * avg_depth # m^3\n",
    "density = 910 # kg/m^3\n",
    "mass = volume * density\n",
    "print(mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500832e-338f-472b-bb1c-56af9e45374c",
   "metadata": {},
   "source": [
    "We have the mass now we need to retreieve velocity data at gz stations to calculate figures for kinetic energy. To do this, for each gz event we need to detect the slip and then calculate the velocity of each station after the event initiates, and do an average of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0089a0df-f8a8-4abb-822b-1291600843ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by loading in data on gz stations from 2011 to 2013\n",
    "df_2011 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2011_2011Events2stas\")\n",
    "df_2012 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2012_2012Events2stas\")\n",
    "df_2013 = my_lib.funcs.load_evt(\"/Users/sambrown04/Documents/SURF/Events/2013_2013Events2stas\")\n",
    "\n",
    "df = df_2011 + df_2012 + df_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01b28c06-ec03-48e6-9ba2-ffa60db1a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = my_lib.funcs.preprocess_events(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a30ca4-3a81-45ed-b40d-8cc4aa5685af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter so it is only gz stations\n",
    "gz_dat = []\n",
    "\n",
    "for i, event in enumerate(df):\n",
    "\n",
    "    filt_df = pd.DataFrame()\n",
    "    \n",
    "    for col in event.columns:\n",
    "        colname = col[0:2]\n",
    "        if colname == 'gz' or colname == 'ti' or colname == 'ti':\n",
    "            column = event[col].copy()\n",
    "            filt_df[col] = column\n",
    "            \n",
    "    gz_dat.append(filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9498ff1-26c6-4faa-a656-f6eb7a4f5db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping event: 17 no valid signals for onset detection.\n",
      "Skipping event: 29 no valid signals for onset detection.\n",
      "Skipping event: 46 no valid signals for onset detection.\n",
      "Skipping event: 52 no valid signals for onset detection.\n",
      "Skipping event: 141 no valid signals for onset detection.\n",
      "Skipping event: 269 no valid signals for onset detection.\n",
      "Skipping event: 360 no valid signals for onset detection.\n",
      "Skipping event: 395 no valid signals for onset detection.\n",
      "Skipping event: 417 no valid signals for onset detection.\n",
      "Skipping event: 422 no valid signals for onset detection.\n",
      "Skipping event: 423 no valid signals for onset detection.\n",
      "Skipping event: 449 no valid signals for onset detection.\n",
      "Skipping event: 459 no valid signals for onset detection.\n",
      "Skipping event: 463 no valid signals for onset detection.\n",
      "Skipping event: 478 no valid signals for onset detection.\n"
     ]
    }
   ],
   "source": [
    "# Loop through each station and calculate the onset of the station\n",
    "\n",
    "f_events = []\n",
    "for i, event in enumerate(gz_dat):\n",
    "    cols = [col for col in event.columns if (col != 'time_sec' and col != 'time_dt')]\n",
    "  \n",
    "    event_onsets = []e \n",
    "    for col in cols:\n",
    "        \n",
    "        # Compute first and second derivatives of the station's signal\n",
    "        grad = my_lib.funcs.derivative(event[col].values)\n",
    "        grad2 = my_lib.funcs.derivative(grad)\n",
    "\n",
    "        # Identify the peak in the second derivative (maximum acceleration)\n",
    "        max_idx = np.argmax(np.abs(grad2))\n",
    "        max_time = event['time_sec'].iloc[max_idx]\n",
    "        event_onsets.append(max_time)\n",
    "        # print(max_time)\n",
    "\n",
    "\n",
    "    if not event_onsets:\n",
    "        print(f\"Skipping event: {i} no valid signals for onset detection.\")\n",
    "        continue  # skip this event\n",
    "    \n",
    "    release_time = max(event_onsets)\n",
    "       \n",
    "    # Splice DataFrame starting from release_time\n",
    "    spliced_event = event[event['time_sec'] >= release_time].reset_index(drop=True)\n",
    "    f_events.append(spliced_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a61df859-8b2d-4328-947c-2ea057817b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "48\n",
      "7\n",
      "7\n",
      "9\n",
      "3\n",
      "11\n",
      "4\n",
      "9\n",
      "2\n",
      "5\n",
      "5\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "derivs_events = []\n",
    "\n",
    "for i, event in enumerate(f_events):\n",
    "    cols_avg = [col for col in event.columns if col not in ['time_sec', 'time_dt']]\n",
    "    f_events[i]['avg_disp'] = event[cols_avg].mean(axis=1)\n",
    "\n",
    "    if f_events[i].shape[0] < 50:\n",
    "        print(f_events[i].shape[0])\n",
    "        continue\n",
    "\n",
    "    # Calculate derivative\n",
    "    deriv = my_lib.funcs.derivative(f_events[i]['avg_disp'])\n",
    "    \n",
    "    # Get corresponding date (first entry in time_dt column)\n",
    "    date = event['time_dt'].iloc[0]\n",
    "\n",
    "    # Append (date, derivative) tuple\n",
    "    derivs_events.append((date, deriv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65254483-a943-4ab7-a5c4-d31a0e73696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_energy(velocity_arr, mass, dt=15):\n",
    "    kinetic_energy = 0.5 * mass * velocity_arr**2  # joules at each step\n",
    "    total_energy = np.sum(kinetic_energy) * dt       # Approx integration\n",
    "    return total_energy  # in joules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64d27d41-9903-4f43-83a7-5388cf8620e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = pd.DataFrame(columns = ['date', 'evt_energy'])\n",
    "\n",
    "dt = 15\n",
    "mass = mass\n",
    "\n",
    "for event in derivs_events:\n",
    "    energies.loc[len(energies)] = {\n",
    "        \"date\": event[0],\n",
    "        \"evt_energy\": event_energy(event[1], mass, dt)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa17c79a-1e9f-4cf4-84ea-60c48291d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = energies.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "960b8cbe-d7a8-41ed-8723-a5839111f915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping event 17: no valid onsets found.\n",
      "Skipping event 29: no valid onsets found.\n",
      "Skipping event 46: no valid onsets found.\n",
      "Skipping event 52: no valid onsets found.\n",
      "Skipping short event 132: only 2 samples.\n",
      "Skipping short event 140: only 48 samples.\n",
      "Skipping event 141: no valid onsets found.\n",
      "Skipping event 269: no valid onsets found.\n",
      "Skipping event 360: no valid onsets found.\n",
      "Skipping event 395: no valid onsets found.\n",
      "Skipping event 417: no valid onsets found.\n",
      "Skipping event 422: no valid onsets found.\n",
      "Skipping event 423: no valid onsets found.\n",
      "Skipping event 449: no valid onsets found.\n",
      "Skipping event 459: no valid onsets found.\n",
      "Skipping event 463: no valid onsets found.\n",
      "Skipping event 478: no valid onsets found.\n",
      "Skipping short event 640: only 7 samples.\n",
      "Skipping short event 726: only 7 samples.\n",
      "Skipping short event 884: only 9 samples.\n",
      "Skipping short event 910: only 3 samples.\n",
      "Skipping short event 943: only 11 samples.\n",
      "Skipping short event 1236: only 4 samples.\n",
      "Skipping short event 1301: only 9 samples.\n",
      "Skipping short event 1441: only 2 samples.\n",
      "Skipping short event 1443: only 5 samples.\n",
      "Skipping short event 1570: only 5 samples.\n",
      "Skipping short event 1574: only 1 samples.\n",
      "Skipping short event 1652: only 2 samples.\n"
     ]
    }
   ],
   "source": [
    "f_events = []\n",
    "derivs_events = []\n",
    "energies = pd.DataFrame(columns=['date', 'evt_energy'])\n",
    "\n",
    "dt = 15  # Time step in seconds\n",
    "mass = mass  # Assumed to be defined earlier\n",
    "\n",
    "for i, event in enumerate(gz_dat):\n",
    "    cols = [col for col in event.columns if col not in ['time_sec', 'time_dt']]\n",
    "    event_onsets = []\n",
    "\n",
    "    for col in cols:\n",
    "        # Compute first and second derivatives\n",
    "        grad = my_lib.funcs.derivative(event[col].values)\n",
    "        grad2 = my_lib.funcs.derivative(grad)\n",
    "\n",
    "        # Identify max of second derivative (absolute value)\n",
    "        max_idx = np.argmax(np.abs(grad2))\n",
    "        max_time = event['time_sec'].iloc[max_idx]\n",
    "        event_onsets.append(max_time)\n",
    "\n",
    "    # Skip if no valid onset found\n",
    "    if not event_onsets:\n",
    "        print(f\"Skipping event {i}: no valid onsets found.\")\n",
    "        continue\n",
    "\n",
    "    # Store date from original (unspliced) event\n",
    "    date = event['time_dt'].iloc[0]\n",
    "\n",
    "    # Determine the release time as the max of onsets\n",
    "    release_time = max(event_onsets)\n",
    "\n",
    "    # Splice the event starting from release_time\n",
    "    spliced_event = event[event['time_sec'] >= release_time].reset_index(drop=True)\n",
    "\n",
    "    # Skip if spliced event too short\n",
    "    if spliced_event.shape[0] < 50:\n",
    "        print(f\"Skipping short event {i}: only {spliced_event.shape[0]} samples.\")\n",
    "        continue\n",
    "\n",
    "    # Compute average displacement\n",
    "    cols_avg = [col for col in spliced_event.columns if col not in ['time_sec', 'time_dt']]\n",
    "    spliced_event['avg_disp'] = spliced_event[cols_avg].mean(axis=1)\n",
    "\n",
    "    # Compute derivative of average displacement\n",
    "    deriv = my_lib.funcs.derivative(spliced_event['avg_disp'])\n",
    "\n",
    "    # Store spliced event\n",
    "    f_events.append(spliced_event)\n",
    "\n",
    "    # Store (date, derivative) pair\n",
    "    derivs_events.append((date, deriv))\n",
    "\n",
    "    # Compute and store energy\n",
    "    evt_E = event_energy(deriv, mass, dt)\n",
    "    energies.loc[len(energies)] = {\n",
    "        \"date\": date,\n",
    "        \"evt_energy\": evt_E\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a8906279-969b-424f-b86c-44f74eb04a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = energies.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "793c1eb4-ae33-459f-b965-d0716f2c4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies.to_csv('Energies_gz', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
