{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc8245e-825c-4474-b1fb-0790fcb631b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sam Brown\n",
    "# Sam_brown@mines.edu\n",
    "# June 20\n",
    "# Goal: Use LSTM neural nets to capture and leverage long and short term patterns in the tidal modulation\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/sambrown04/Documents/SURF/whillans-surf/notebooks/SURF\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "df = pd.read_csv(\"/Users/sambrown04/Documents/SURF/Preproc_data/10-18.csv\", parse_dates=[\"start_time\"])\n",
    "df = df.iloc[500:3000] # Account for gaps in time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cea247d-9f41-4523-8cd2-199efd981f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Target\n",
    "X = df[['tide_height', 'tide_deriv', 'A_diurn', 'A_semidiurn', 'high_t_evt', 'time_since', 'sev_stds', 'pre-s_stds', 'form_fac']]\n",
    "y = df['slip_size_standardized'].values.reshape(-1,1)\n",
    "\n",
    "# Normalize features\n",
    "x_scaler = StandardScaler()\n",
    "X_scaled = x_scaler.fit_transform(X)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaled = y_scaler.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cae3444d-6ae9-4357-a1d9-fe35aeb310df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seq shape: torch.Size([2470, 30, 9])\n",
      "y_seq shape: torch.Size([2470, 1])\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN =  30 # Sequence of \"memory\"\n",
    "batch_size = 32\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "\n",
    "# We Want to create rolling sequences of SEQ_LEN\n",
    "for i in range(len(X_scaled) - SEQ_LEN):\n",
    "    seq = X_scaled[i:i+SEQ_LEN]\n",
    "    target = y_scaled[i + SEQ_LEN - 1][0]  # flatten from 2D array NOTE: To change which target we predict remove -1 -> predicts target after last row of features.\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "#Tensors\n",
    "X_seq = torch.tensor(np.array(sequences), dtype=torch.float32)\n",
    "y_seq = torch.tensor(np.array(targets), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(\"X_seq shape:\", X_seq.shape)  # [num_samples, seq_len, 6]\n",
    "print(\"y_seq shape:\", y_seq.shape)  # [num_samples, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e9bd20b-9ef4-49e4-90c7-4e124bad0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test to prevent data leakage\n",
    "train_size = int(0.8 * len(X_seq))\n",
    "\n",
    "X_train = X_seq[:train_size]\n",
    "y_train = y_seq[:train_size]\n",
    "X_test = X_seq[train_size:]\n",
    "y_test = y_seq[train_size:]\n",
    "\n",
    "# Create DataLoaders with no shuffling\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa93b1b-7e0a-4eda-bd80-6a2410f13310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class SlipLSTM(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=64, num_layers=1):\n",
    "        super(SlipLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3) # Dropout, saw on forum... prevents overfitting\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)       # out: batch, seq_len, hidden_size\n",
    "        out = out[:, -1, :]         # take output at last time step\n",
    "        out = self.fc1(out)         # linear layer 1\n",
    "        out = self.relu(out)        # ReLU activation\n",
    "        out = self.fc2(out)         # final output layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7e6cd1-113a-4dc5-b47a-3ed7ab18ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SlipLSTM(input_size=9, hidden_size=64, num_layers=1)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4379ba2d-305a-4587-880f-78a7adfcfcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6474\n",
      "Epoch 2/30, Loss: 0.1781\n",
      "Epoch 3/30, Loss: 0.1264\n",
      "Epoch 4/30, Loss: 0.1011\n",
      "Epoch 5/30, Loss: 0.0836\n",
      "Epoch 6/30, Loss: 0.0726\n",
      "Epoch 7/30, Loss: 0.0658\n",
      "Epoch 8/30, Loss: 0.0605\n",
      "Epoch 9/30, Loss: 0.0574\n",
      "Epoch 10/30, Loss: 0.0563\n",
      "Epoch 11/30, Loss: 0.0561\n",
      "Epoch 12/30, Loss: 0.0537\n",
      "Epoch 13/30, Loss: 0.0515\n",
      "Epoch 14/30, Loss: 0.0505\n",
      "Epoch 15/30, Loss: 0.0497\n",
      "Epoch 16/30, Loss: 0.0490\n",
      "Epoch 17/30, Loss: 0.0486\n",
      "Epoch 18/30, Loss: 0.0482\n",
      "Epoch 19/30, Loss: 0.0478\n",
      "Epoch 20/30, Loss: 0.0471\n",
      "Epoch 21/30, Loss: 0.0470\n",
      "Epoch 22/30, Loss: 0.0466\n",
      "Epoch 23/30, Loss: 0.0463\n",
      "Epoch 24/30, Loss: 0.0458\n",
      "Epoch 25/30, Loss: 0.0452\n",
      "Epoch 26/30, Loss: 0.0436\n",
      "Epoch 27/30, Loss: 0.0429\n",
      "Epoch 28/30, Loss: 0.0429\n",
      "Epoch 29/30, Loss: 0.0413\n",
      "Epoch 30/30, Loss: 0.0397\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y in train_loader: # mini- batch loop\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_X)\n",
    "        loss = loss_fn(preds, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d51ace-8b78-4a66-96f2-f5b6f78252ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.4846\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "preds, trues = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        pred = model(batch_X)\n",
    "        preds.append(pred.numpy())\n",
    "        trues.append(batch_y.numpy())\n",
    "\n",
    "preds = np.vstack(preds)\n",
    "trues = np.vstack(trues)\n",
    "\n",
    "mse = mean_squared_error(trues, preds)\n",
    "print(f\"Test MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a8fd7d6-4104-4cd9-baad-d3490a8c83a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score: 0.522799015045166\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(trues, preds)\n",
    "print(\"R² score:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
